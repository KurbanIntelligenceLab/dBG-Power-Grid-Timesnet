{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55b453fab8a4c80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_provider.m4 import M4Dataset\n",
    "\n",
    "seasonal_patterns = 'Monthly'\n",
    "m4 = M4Dataset.load(training=True, dataset_file='./dataset/m4')\n",
    "training_values = np.array(\n",
    "    [v[~np.isnan(v)] for v in m4.values[m4.groups == seasonal_patterns]])  # split different frequencies\n",
    "ids = np.array([i for i in m4.ids[m4.groups == seasonal_patterns]])\n",
    "data = [ts for ts in training_values]\n",
    "flat_data = np.concatenate(data)\n",
    "\n",
    "print('Dataset size: ', len(data))\n",
    "print('Flat Dataset size: ', len(flat_data))\n",
    "\n",
    "\n",
    "def create_bin_count_table(array, bin_ranges):\n",
    "    # Initialize bins\n",
    "    bins = [-np.inf] + bin_ranges + [np.inf]\n",
    "\n",
    "    # Calculate histogram bin counts\n",
    "    bin_counts, _ = np.histogram(array, bins=bins)\n",
    "\n",
    "    # Create a DataFrame for the table\n",
    "    bin_labels = [f\"Bin {i + 1}\" for i in range(len(bin_counts))]\n",
    "    histogram_df = pd.DataFrame({\n",
    "        'Bin': bin_labels,\n",
    "        'Range': [f\"<= {bin_ranges[0]}\" if i == 0 else\n",
    "                  f\">= {bin_ranges[-1]}\" if i == len(bin_counts) - 1 else\n",
    "                  f\"{bin_ranges[i - 1]} < n <= {bin_ranges[i]}\" for i in range(len(bin_counts))],\n",
    "        'Count': bin_counts\n",
    "    })\n",
    "\n",
    "    return histogram_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manual Discretization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91cc502f0f7bbfc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Calculate histogram bin counts without plotting\n",
    "bin_counts, bin_edges = np.histogram(flat_data, bins=50)\n",
    "\n",
    "# Create a table of bin counts\n",
    "histogram_table = np.column_stack((bin_edges[:-1], bin_edges[1:], bin_counts))\n",
    "\n",
    "# Convert the table to a more readable format\n",
    "histogram_df = pd.DataFrame(histogram_table, columns=['Bin Start', 'Bin End', 'Count'])\n",
    "\n",
    "ranges = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1200, 1400, 1600, 1800, 2000,\n",
    "          2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 12000, 14000, 16000, 18000, 20000, 30000]\n",
    "\n",
    "histogram_df = create_bin_count_table(flat_data, ranges)\n",
    "histogram_df\n",
    "\n",
    "\n",
    "# discretizer = Discretization(ranges)\n",
    "# print('Descretizing data')\n",
    "# discretized_data = [[discretizer.discretize(value) for value in element] for element in data]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d3ce20161f7843b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0fab1fe7b56f090"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from pyts.approximation import SymbolicAggregateApproximation\n",
    "\n",
    "# SAX transformation\n",
    "n_bins = 25\n",
    "sax = SymbolicAggregateApproximation(n_bins=n_bins, strategy='normal')\n",
    "X_sax = sax.fit_transform(np.array(flat_data).reshape(1, -1))\n",
    "\n",
    "# Compute gaussian bins\n",
    "bins = norm.ppf(np.linspace(0, 1, n_bins + 1)[1:-1])\n",
    "\n",
    "# Show the results for the first time series\n",
    "bottom_bool = np.r_[True, X_sax[0, 1:] > X_sax[0, :-1]]\n",
    "\n",
    "print(bins)\n",
    "\n",
    "# Count unique values\n",
    "unique_values, counts = np.unique(X_sax, return_counts=True)\n",
    "\n",
    "# Displaying unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3c37a9be94523e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "SAX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0b9b3ee4a7e952"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from saxpy.znorm import znorm\n",
    "from saxpy.sax import ts_to_string\n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "\n",
    "dat_znorm = znorm(flat_data)\n",
    "#dat_paa_3 = paa(dat_znorm, len(dat_znorm))\n",
    "print(dat_znorm)\n",
    "\n",
    "sax_flat_data = ts_to_string(dat_znorm, cuts_for_asize(20))\n",
    "char_to_int = lambda c: ord(c) - ord('a')\n",
    "sax_flat_data = [char_to_int(char) for char in sax_flat_data]\n",
    "sax_flat_data = np.array(sax_flat_data)\n",
    "\n",
    "original_shapes = [array.shape for array in data]\n",
    "reshaped_data = []\n",
    "start = 0\n",
    "for shape in original_shapes:\n",
    "    end = start + shape[0]\n",
    "    reshaped_data.append(sax_flat_data[start:end].tolist())\n",
    "    start = end\n",
    "\n",
    "print(len(reshaped_data))\n",
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7015cbf42b09370",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dBG.FeatureGraph import FeatureGraph\n",
    "from dBG.utils.Substitute.LogOdds import LogOdds\n",
    "\n",
    "sub_matrix = LogOdds(reshaped_data)\n",
    "\n",
    "dbg = FeatureGraph(4, reshaped_data, approximate=True, substitute=sub_matrix, similarity_threshold=0.003)\n",
    "print(dbg)\n",
    "features = dbg.generate_features(0.01)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d027b8b1935873e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 231\n",
    "sim_thr = 0.003\n",
    "\n",
    "larger_than_number = sub_matrix.similarity_matrix > sim_thr\n",
    "len(sub_matrix.similarity_matrix[larger_than_number])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adaed20126f091bb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove similar features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e1705135774e1b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "\n",
    "print(len(features))\n",
    "\n",
    "\n",
    "def tuple_to_string(t):\n",
    "    return ''.join(chr(i) for i in t)\n",
    "\n",
    "\n",
    "unique_features = []\n",
    "for feat1 in features:\n",
    "    is_similar = False\n",
    "    for feat2 in unique_features:\n",
    "        sim_thr = max(len(feat1), len(feat2)) * 0.5\n",
    "        if lev(tuple_to_string(feat1), tuple_to_string(feat2)) < sim_thr:\n",
    "            is_similar = True\n",
    "            break\n",
    "    if not is_similar:\n",
    "        unique_features.append(feat1)\n",
    "\n",
    "print(len(unique_features))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff38c711168a76fd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Allignment Alg"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac78110d93ae6f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import swalign\n",
    "from tqdm.contrib.concurrent import process_map  # Import process_map from tqdm.contrib.concurrent\n",
    "\n",
    "match = 2\n",
    "mismatch = -1\n",
    "scoring = swalign.NucleotideScoringMatrix(match, mismatch)\n",
    "sw = swalign.LocalAlignment(scoring)  # you can also choose gap penalties, etc...\n",
    "\n",
    "\n",
    "def tuple_to_string(t):\n",
    "    return ''.join(chr(i) for i in t)\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    row_str = tuple_to_string(row)\n",
    "    csv_row = [row]  # Start with the original int tuple\n",
    "    for feature in unique_features:\n",
    "        feature_str = tuple_to_string(feature)\n",
    "        score = sw.align(row_str, feature_str).score\n",
    "        csv_row.append(score)\n",
    "    return csv_row\n",
    "\n",
    "\n",
    "# Use process_map for parallel processing with a progress bar\n",
    "results = list(process_map(process_row, reshaped_data, max_workers=None, chunksize=1, total=len(reshaped_data)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407f5ac831cc4273",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataset/Disc/reshaped_data.pkl', 'wb') as file:\n",
    "    # Use pickle to serialize and save the data\n",
    "    pickle.dump(reshaped_data, file)\n",
    "\n",
    "with open('dataset/Disc/features.pkl', 'wb') as file:\n",
    "    # Use pickle to serialize and save the data\n",
    "    pickle.dump(features, file)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64ac3554a0c7c98c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lev_features = pd.read_csv('dataset/Disc/all_features_weighted_lev_dist.csv')\n",
    "sampled_df = lev_features.sample(frac=0.1)\n",
    "\n",
    "print(lev_features.shape)\n",
    "print(sampled_df.shape)\n",
    "\n",
    "sampled_df = sampled_df.drop('Data', axis=1)\n",
    "correlation_matrix = sampled_df.corr()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f38ae79e0c0cf425",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = correlation_matrix.columns.tolist()\n",
    "\n",
    "# Iteratively select features\n",
    "for _ in range(k):\n",
    "    min_corr = float('inf')\n",
    "    best_feature = None\n",
    "\n",
    "    for feature in remaining_features:\n",
    "        # Calculate the average correlation of the feature with the already selected features\n",
    "        if selected_features:\n",
    "            avg_corr = correlation_matrix.loc[selected_features, feature].abs().mean()\n",
    "        else:\n",
    "            # For the first feature, use the sum of correlations with all other features\n",
    "            avg_corr = correlation_matrix[feature].abs().sum() - 1  # subtract self-correlation\n",
    "\n",
    "        if avg_corr < min_corr:\n",
    "            min_corr = avg_corr\n",
    "            best_feature = feature\n",
    "\n",
    "    # Add the best feature to the selected list and remove it from the remaining list\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "\n",
    "for feat in selected_features:\n",
    "    print(feat)\n",
    "print(len(selected_features))\n",
    "with open('dataset/Disc/unique_features.txt', 'w') as file:\n",
    "    for feature in selected_features:\n",
    "        file.write(feature + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19412b3591748e87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import joblib\n",
    "from collections import Counter\n",
    "from data_provider.m4 import M4Dataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from dBG.utils.Substitute.LogOdds import LogOdds\n",
    "from dBG.FeatureGraph import FeatureGraph\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "similarity_threshold = {\n",
    "    'Yearly': 0.08,\n",
    "    'Quarterly': 0.025,\n",
    "    'Monthly': 0.003,\n",
    "    'Weekly': 0,\n",
    "    'Daily': -0.2,\n",
    "    'Hourly': -0.4\n",
    "}\n",
    "\n",
    "seasonal_patterns = ['Monthly', 'Quarterly', 'Yearly', 'Weekly', 'Daily', 'Hourly']\n",
    "\n",
    "\n",
    "def bin_descretize(sequence, discretizer):\n",
    "    flat_data = np.concatenate(sequence)\n",
    "    discrete_time_sequence = discretizer.transform(flat_data.reshape(-1, 1))\n",
    "\n",
    "    discrete_time_sequence = discrete_time_sequence.flatten().astype(int)\n",
    "    # Count the frequency of each value in the discretized data\n",
    "    distribution = Counter(discrete_time_sequence)\n",
    "\n",
    "    # Convert the distribution to a more readable format\n",
    "    print(dict(distribution))\n",
    "    original_shapes = [array.shape for array in sequence]\n",
    "    reshaped_data = []\n",
    "    start = 0\n",
    "    for shape in original_shapes:\n",
    "        end = start + shape[0]\n",
    "        reshaped_data.append(discrete_time_sequence[start:end].tolist())\n",
    "        start = end\n",
    "    return reshaped_data\n",
    "\n",
    "\n",
    "for pattern in seasonal_patterns:\n",
    "    print(f'Reading {pattern} data...')\n",
    "    m4 = M4Dataset.load(training=True, dataset_file='dataset/m4')\n",
    "    training_values = np.array([v[~np.isnan(v)] for v in m4.values[m4.groups == pattern]])\n",
    "    data = [ts for ts in training_values]\n",
    "    discretizer = joblib.load(f'dataset/Discretizer/20Disc/{pattern}_discretizer_model.joblib')\n",
    "    data = bin_descretize(data, discretizer)\n",
    "    print(len(data))\n",
    "    print('Dataset size: ', len(data))\n",
    "    sub_matrix = LogOdds(data)\n",
    "    dbg = FeatureGraph(k=3, sequences=data, approximate=True, substitute=sub_matrix,\n",
    "                       similarity_threshold=similarity_threshold[pattern])\n",
    "    G = dbg.graph\n",
    "    print(G)\n",
    "    node_label_mapping = {node: i for i, node in enumerate(G.nodes)}\n",
    "    G = nx.relabel_nodes(G, node_label_mapping)\n",
    "    print('Saving graph...')\n",
    "    with open(f'dataset/Graphs/{pattern}_edges.txt', 'w') as f:\n",
    "        for u, v, data in tqdm(G.edges(data=True)):\n",
    "            for i in range(int(G[u][v].get('weight', 1))):\n",
    "                f.write(f\"{u} {v}\\n\")\n",
    "\n",
    "    # Save the node label mapping to a separate file\n",
    "    mapping_file_path = f'dataset/Graphs/{pattern}_nodes.joblib'\n",
    "    joblib.dump(node_label_mapping, mapping_file_path)\n",
    "    print(f'Node label mapping saved to {mapping_file_path}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4faeb46ed9c79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Path to the directory you want to clean\n",
    "directory_path = '/run/media/lumpus/HDD Storage/PycharmProjects/Time-Series-Library/struc2vec-master/pickles'\n",
    "\n",
    "seasonal_patterns = ['Daily', 'Hourly', 'Yearly', 'Quarterly', 'Monthly', 'Weekly']\n",
    "\n",
    "for pattern in seasonal_patterns:\n",
    "    print(f'Reading {pattern} data...')\n",
    "\n",
    "    # Define the command and parameters\n",
    "    command = 'venv/bin/python'\n",
    "    script_path = 'struc2vec-master/src/main.py'\n",
    "    params = [\n",
    "        '--input',\n",
    "        f'/run/media/lumpus/HDD Storage/PycharmProjects/Time-Series-Library/dataset/Graphs/{pattern}_edges.txt',\n",
    "        '--output',\n",
    "        f'/run/media/lumpus/HDD Storage/PycharmProjects/Time-Series-Library/dataset/graph_emb/{pattern}.emb',\n",
    "        '--weighted',\n",
    "        '--directed',\n",
    "        '--workers', '24',\n",
    "        '--dimensions', '32',\n",
    "        '--OPT1', 'true',\n",
    "        '--OPT1', 'true',\n",
    "        '--OPT3', 'true'\n",
    "    ]\n",
    "\n",
    "    # Combine command and parameters\n",
    "    full_command = [command, script_path] + params\n",
    "\n",
    "    start = time.time()\n",
    "    # Run the command\n",
    "    subprocess.run(full_command)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Time Cost {(end - start):.2f}s')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99bf1d9f89bede62",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " with open('/run/media/lumpus/HDD Storage/PycharmProjects/Time-Series-Library/dataset/graph_emb/Yearly.emb',\n",
    "           'r') as file:\n",
    "    first_line = file.readline()\n",
    "    specified_dimensions = list(map(int, first_line.split()))\n",
    "\n",
    "print(f)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f7b2ff038ca92d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from data_provider.m4 import M4Dataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from dBG.utils.Substitute.LogOdds import LogOdds\n",
    "from dBG.FeatureGraph import FeatureGraph\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    " \n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "similarity_threshold = {\n",
    "    'Yearly': 0.08,\n",
    "    'Quarterly': 0.025,\n",
    "    'Monthly': 0.003,\n",
    "    'Weekly': 0,\n",
    "    'Daily': -0.2,\n",
    "    'Hourly': -0.4\n",
    "}\n",
    " \n",
    "seasonal_patterns = ['Monthly', 'Quarterly', 'Yearly', 'Weekly', 'Daily', 'Hourly']\n",
    "\n",
    "sample = {\n",
    "    'Monthly': 2, \n",
    "    'Quarterly': 65, \n",
    "    'Yearly': 72, \n",
    "    'Weekly': 3, \n",
    "    'Daily': 10, \n",
    "    'Hourly': 124\n",
    "}\n",
    " \n",
    "def bin_descretize(sequence, discretizer):\n",
    "    flat_data = np.concatenate(sequence)\n",
    "    discrete_time_sequence = discretizer.fit_transform(flat_data.reshape(-1, 1))\n",
    " \n",
    "    discrete_time_sequence = discrete_time_sequence.flatten().astype(int)\n",
    "    # Count the frequency of each value in the discretized data\n",
    "    distribution = Counter(discrete_time_sequence)\n",
    " \n",
    "    # Convert the distribution to a more readable format\n",
    "    print(dict(distribution))\n",
    "    original_shapes = [array.shape for array in sequence]\n",
    "    reshaped_data = []\n",
    "    start = 0\n",
    "    for shape in original_shapes:\n",
    "        end = start + shape[0]\n",
    "        reshaped_data.append(discrete_time_sequence[start:end].tolist())\n",
    "        start = end\n",
    "    return reshaped_data\n",
    " \n",
    " # 578\n",
    "for pattern in seasonal_patterns:\n",
    "    print(f'Reading {pattern} data...')\n",
    "    m4 = M4Dataset.load(training=True, dataset_file='dataset/m4')\n",
    "    training_values = [v[~np.isnan(v)] for v in m4.values[m4.groups == pattern]]\n",
    "    data = [ts for ts in training_values]\n",
    "    discretizer = KBinsDiscretizer(n_bins=20, encode='ordinal', strategy='quantile')\n",
    "    data = bin_descretize(data, discretizer)\n",
    "    # joblib.dump(discretizer, f'dataset/Discretizer/20Disc/{pattern}_discretizer_model.joblib')\n",
    "    # data = data[:sample[pattern]]\n",
    "    print(len(data))\n",
    "    print('Dataset size: ', len(data))\n",
    "    sub_matrix = LogOdds(data)\n",
    "    dbg = FeatureGraph(k=5, sequences=data, approximate=True, substitute=sub_matrix,\n",
    "                       similarity_threshold=similarity_threshold[pattern])\n",
    "    G = dbg.graph\n",
    "    print(G)\n",
    "\n",
    "    exit(-1)\n",
    " \n",
    "    # Export Node List\n",
    "    node_data = pd.DataFrame([[t] for t in G.nodes()], columns=['Node'])\n",
    "    node_data['Max'] = node_data['Node'].apply(lambda x: max(x))\n",
    "    node_data['Node'] = node_data['Node'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    node_data.to_csv(f'dataset/Cythospace/{pattern}_nodelist.csv', index=False)\n",
    " \n",
    "    # Export Edge List\n",
    "    # Get edge list with all attributes\n",
    "    edge_data = G.edges(data=True)\n",
    "    \n",
    "    # Convert edge list to pandas DataFrame dynamically\n",
    "    # This will create a list of dictionaries where each dictionary corresponds to an edge and its attributes\n",
    "    edges_list = [{'Source': u, 'Target': v, **data} for u, v, data in edge_data]\n",
    "    \n",
    "    # Now convert this list of dictionaries to a DataFrame\n",
    "    edge_df = pd.DataFrame(edges_list)\n",
    "    edge_df = edge_df.drop('substitute_edges', axis=1)\n",
    "    edge_df['Max'] = edge_df['tuple'].apply(lambda x: max(x))\n",
    "    edge_df['Source'] = edge_df['Source'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    edge_df['Target'] = edge_df['Target'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    edge_df['tuple'] = edge_df['tuple'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    edge_df.to_csv(f'dat|aset/Cythospace/{pattern}_edgelist.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a78f45e07bf741",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Adjusted code to use subfigures for visualization of all seasons in a single image\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10)) # 2 rows, 3 columns layout for the subplots\n",
    "axes = axes.flatten() # Flatten the axes array for easy iteration\n",
    "\n",
    "# Mock data and cluster numbers for illustration; replace with actual data loading and clustering\n",
    "# clusters dictionary as provided\n",
    "clusters = {\n",
    "    \"Weekly\": 6,\n",
    "    \"Daily\": 14,\n",
    "    \"Quarterly\": 6,\n",
    "    \"Hourly\": 14,\n",
    "    \"Yearly\": 14,\n",
    "    \"Monthly\": 3\n",
    "}\n",
    "\n",
    "seasons = [\"Weekly\", \"Daily\", \"Quarterly\", \"Hourly\", \"Yearly\", \"Monthly\"]\n",
    "\n",
    "# Placeholder for actual data loading and clustering\n",
    "# Since the environment doesn't have access to 'dataset/graph_emb/{season}.emb', we'll simulate the data\n",
    "# For demonstration, simulate 400 samples with 32 dimensions as in the original question\n",
    "np.random.seed(0) # For reproducible random data\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    data = np.loadtxt(f'dataset/graph_emb/{season}.emb', skiprows=1, usecols=range(1, 33))\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    n_clusters = clusters[season]\n",
    "    kmeans = KMeans(n_init='auto', n_clusters=n_clusters, random_state=0).fit(data)\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction to 2D for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    data_2d = tsne.fit_transform(data)\n",
    "\n",
    "    # Visualization in the subplot\n",
    "    ax = axes[i]\n",
    "    for j in range(n_clusters):\n",
    "        ax.scatter(data_2d[kmeans.labels_ == j, 0], data_2d[kmeans.labels_ == j, 1], label=f'Cluster {j+1}', alpha=0.7, edgecolor='w', s=50)\n",
    "    ax.set_title(f'{season}',fontsize=24)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "# plt.tight_layout()\n",
    "#plt.savefig('Images/scatter.pdf')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cef7ba1a8443913",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1178d30f90e0786c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
